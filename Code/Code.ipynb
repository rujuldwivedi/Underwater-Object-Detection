{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnderwaterDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.img_dir = os.path.join(root_dir, split)\n",
    "        self.label_dir = os.path.join(root_dir, f\"{split}_labels\")\n",
    "        self.img_files = [f for f in os.listdir(self.img_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.img_files[idx].replace('.jpg', '.txt'))\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(label_path, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                data = line.strip().split()\n",
    "                label = int(data[0])\n",
    "                x_center, y_center, width, height = map(float, data[1:])\n",
    "                xmin = x_center - width / 2\n",
    "                ymin = y_center - height / 2\n",
    "                xmax = x_center + width / 2\n",
    "                ymax = y_center + height / 2\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                labels.append(label)\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, {'boxes': boxes, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBox Prediction: torch.Size([2, 4]), Class Prediction: torch.Size([2, 7])\n"
     ]
    }
   ],
   "source": [
    "class LightViT(nn.Module):\n",
    "    def __init__(self, num_classes=7, image_size=256, patch_size=16, dim=128, depth=4, heads=4, mlp_dim=256):\n",
    "        super(LightViT, self).__init__()\n",
    "        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n",
    "\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = 3 * patch_size ** 2\n",
    "\n",
    "        # Patch Embedding\n",
    "        self.patch_embedding = nn.Linear(patch_dim, dim)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, num_patches, dim))\n",
    "\n",
    "        # Transformer Encoder Layers\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(dim, heads, mlp_dim),\n",
    "            num_layers=depth\n",
    "        )\n",
    "\n",
    "        # MLP Head for Bounding Box Prediction\n",
    "        self.to_bbox = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, 4)\n",
    "        )\n",
    "\n",
    "        # MLP Head for Classification\n",
    "        self.to_class = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, _, _ = x.shape\n",
    "\n",
    "        # Split image into patches\n",
    "        x = x.view(batch_size, 3, -1, 16, 16).permute(0, 2, 1, 3, 4).contiguous().view(batch_size, -1, 16 * 16 * 3)\n",
    "\n",
    "        # Patch Embedding\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # Add positional encoding\n",
    "        x += self.positional_encoding\n",
    "\n",
    "        # Transformer encoding\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Bounding box prediction and classification\n",
    "        bbox_pred = self.to_bbox(x.mean(dim=1))\n",
    "        class_pred = self.to_class(x.mean(dim=1))\n",
    "\n",
    "        return bbox_pred, class_pred\n",
    "\n",
    "# Example Usage\n",
    "model_vit = LightViT(num_classes=7)\n",
    "sample_image = torch.rand((2, 3, 256, 256))  # Batch of 2 images\n",
    "bbox_pred, class_pred = model_vit(sample_image)\n",
    "print(f\"BBox Prediction: {bbox_pred.shape}, Class Prediction: {class_pred.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBox Prediction: torch.Size([2, 4]), Class Prediction: torch.Size([2, 7])\n"
     ]
    }
   ],
   "source": [
    "class LightYOLO(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(LightYOLO, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),  # Downsample\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Prediction heads\n",
    "        self.bbox_head = nn.Linear(128 * 16 * 16, 4)\n",
    "        self.class_head = nn.Linear(128 * 16 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        # Bounding box prediction\n",
    "        bbox_pred = self.bbox_head(x)\n",
    "        class_pred = self.class_head(x)\n",
    "\n",
    "        return bbox_pred, class_pred\n",
    "\n",
    "# Example Usage\n",
    "model_yolo = LightYOLO(num_classes=7)\n",
    "bbox_pred, class_pred = model_yolo(sample_image)\n",
    "print(f\"BBox Prediction: {bbox_pred.shape}, Class Prediction: {class_pred.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBox Prediction: torch.Size([2, 10, 4]), Class Prediction: torch.Size([2, 10, 7])\n"
     ]
    }
   ],
   "source": [
    "class LightViT_YOLO(nn.Module):\n",
    "    def __init__(self, num_classes=7, max_objects=10):\n",
    "        super(LightViT_YOLO, self).__init__()\n",
    "        self.max_objects = max_objects\n",
    "        self.vit = LightViT(num_classes)\n",
    "        self.yolo = LightYOLO(num_classes)\n",
    "        self.final_bbox_head = nn.Linear(8, 4 * max_objects)\n",
    "        self.final_class_head = nn.Linear(14, num_classes * max_objects)\n",
    "\n",
    "    def forward(self, x):\n",
    "        vit_bbox, vit_class = self.vit(x)\n",
    "        yolo_bbox, yolo_class = self.yolo(x)\n",
    "        combined_bbox = torch.cat((vit_bbox, yolo_bbox), dim=1)\n",
    "        combined_class = torch.cat((vit_class, yolo_class), dim=1)\n",
    "        final_bbox = self.final_bbox_head(combined_bbox).view(-1, self.max_objects, 4)\n",
    "        final_class = self.final_class_head(combined_class).view(-1, self.max_objects, 7)\n",
    "        return final_bbox, final_class\n",
    "    \n",
    "    # Example Usage\n",
    "model_combined = LightViT_YOLO(num_classes=7)\n",
    "bbox_pred, class_pred = model_combined(sample_image)\n",
    "print(f\"BBox Prediction: {bbox_pred.shape}, Class Prediction: {class_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    \n",
    "    for item in batch:\n",
    "        images.append(item[0])\n",
    "        boxes.append(item[1]['boxes'])\n",
    "        labels.append(item[1]['labels'])\n",
    "\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    max_objects = max(len(b) for b in boxes)\n",
    "    padded_boxes = []\n",
    "    padded_labels = []\n",
    "\n",
    "    for b, l in zip(boxes, labels):\n",
    "        if len(b) < max_objects:\n",
    "            pad_size = max_objects - len(b)\n",
    "            padded_boxes.append(torch.cat([b, torch.zeros(pad_size, 4)]))\n",
    "            padded_labels.append(torch.cat([l, torch.zeros(pad_size, dtype=torch.long)]))\n",
    "        else:\n",
    "            padded_boxes.append(b[:max_objects])\n",
    "            padded_labels.append(l[:max_objects])\n",
    "\n",
    "    return images, {\n",
    "        'boxes': torch.stack(padded_boxes),\n",
    "        'labels': torch.stack(padded_labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs=100):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, targets in train_loader:\n",
    "            images = images.to(device)\n",
    "            bbox_targets = targets['boxes'].to(device)\n",
    "            class_targets = targets['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            bbox_pred, class_pred = model(images)\n",
    "\n",
    "            # Ensure bbox_pred and bbox_targets have the same shape\n",
    "            max_objects = bbox_targets.size(1)\n",
    "            bbox_pred = bbox_pred[:, :max_objects, :]\n",
    "\n",
    "            # Calculate losses\n",
    "            loss_bbox = nn.functional.mse_loss(bbox_pred, bbox_targets)\n",
    "            loss_class = criterion(class_pred[:, :max_objects, :].reshape(-1, 7), class_targets.reshape(-1))\n",
    "            loss = loss_bbox + loss_class\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images = images.to(device)\n",
    "            bbox_pred, class_pred = model(images)\n",
    "            \n",
    "            for i in range(len(images)):\n",
    "                # Get the number of actual objects in this image\n",
    "                num_objects = (targets['labels'][i] != 0).sum().item()\n",
    "                \n",
    "                # Only compare predictions for actual objects\n",
    "                pred_classes = class_pred[i, :num_objects].argmax(dim=1)\n",
    "                true_classes = targets['labels'][i, :num_objects].to(device)\n",
    "                \n",
    "                total += num_objects\n",
    "                correct += (pred_classes == true_classes).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(image, bbox_pred, class_pred, class_names):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "    for bbox, cls_scores in zip(bbox_pred, class_pred):\n",
    "        if bbox.sum() > 0:  # Only plot non-zero boxes\n",
    "            xmin, ymin, xmax, ymax = bbox.cpu().numpy()\n",
    "            rect = patches.Rectangle((xmin*256, ymin*256), (xmax-xmin)*256, (ymax-ymin)*256, \n",
    "                                     linewidth=1, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            cls_name = class_names[cls_scores.argmax()]\n",
    "            plt.text(xmin*256, ymin*256, cls_name, fontsize=8, color='r')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = UnderwaterDataset(root_dir='data/USIS10K', split='train', transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    model = LightViT_YOLO(num_classes=7, max_objects=10)\n",
    "    train_model(model, train_loader)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'multi_object_model.pth')\n",
    "\n",
    "    # Evaluation\n",
    "    test_dataset = UnderwaterDataset(root_dir='data/USIS10K', split='test', transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    evaluate_model(model, test_loader)\n",
    "\n",
    "    # Visualization\n",
    "    class_names = [\"wrecks/ruins\", \"fish\", \"reefs\", \"aquatic plants\", \"human divers\", \"robots\", \"sea-floor\"]\n",
    "    model.eval()\n",
    "    \n",
    "    for images, targets in test_loader:\n",
    "        images = images.to(device)\n",
    "        bbox_pred, class_pred = model(images)\n",
    "        \n",
    "        for i in range(10):\n",
    "            num_objects = (targets['labels'][i] != 0).sum().item()\n",
    "            \n",
    "            \n",
    "            plot_predictions(images[i], bbox_pred[i, :num_objects].detach(), class_pred[i, :num_objects].detach(), class_names)\n",
    "            input(\"Press Enter to continue...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
